# Modern benchmark workflow with improved dependency management and caching
name: Benchmark

on:
  push:
    branches: ["master", "main"]
  pull_request:
    branches: ["master", "main"]
  schedule:
    # Run benchmarks weekly on Sundays at 02:00 UTC
    - cron: "0 2 * * 0"
  workflow_dispatch:
    inputs:
      python-version:
        description: "Python version to test"
        required: false
        default: "3.11"
        type: choice
        options:
          - "3.11"
          - "3.12"
          - "3.13"

env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1

jobs:
  benchmark:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.11", "3.12", "3.13"]
        exclude:
          # Reduce matrix size for faster CI, can be enabled if needed
          - os: macos-latest
            python-version: "3.12"
          - os: windows-latest
            python-version: "3.12"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pipenv
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-

      - name: Install system dependencies (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential gcc g++ gfortran libopenblas-dev liblapack-dev

      - name: Install system dependencies (macOS)
        if: matrix.os == 'macos-latest'
        run: |
          brew install gcc openblas lapack
          # Set environment variables for OpenMP support
          echo "CC=/usr/local/bin/gcc-11" >> $GITHUB_ENV
          echo "CXX=/usr/local/bin/g++-11" >> $GITHUB_ENV

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          python -m pip install tabulate matplotlib seaborn pandas
          python -m pip install "numpy>=1.21.0,<3.0" "scipy>=1.6.0" "joblib>=1.2.0"

      - name: Clean previous builds
        run: |
          python -c "
          import shutil, pathlib
          for path in ['build', 'dist']:
              p = pathlib.Path(path)
              if p.exists():
                  shutil.rmtree(p)
          for path in pathlib.Path('.').rglob('*.egg-info'):
              if path.is_dir():
                  shutil.rmtree(path)
          for path in pathlib.Path('surprise').rglob('*.so'):
              if path.is_file():
                  path.unlink()
          "

      - name: Install Surprise with optimizations
        shell: bash
        run: |
          # Set optimization flags
          if [[ "${{ runner.os }}" == "Linux" || "${{ runner.os }}" == "macOS" ]]; then
            export CFLAGS="-O3 -ffast-math -march=native"
            export CXXFLAGS="-O3 -ffast-math -march=native"
          fi

          # Install with no cache to ensure fresh build
          pip install --no-cache-dir --force-reinstall -e .

      - name: Verify installation
        run: |
          python -c "
          import surprise
          print(f'Surprise version: {surprise.__version__}')
          from surprise import SVD, Dataset
          print('✓ Basic imports successful')
          "

      - name: Run setup verification tests
        run: |
          python test_setup.py

      - name: Run quick verification benchmark
        run: |
          python -c "
          import time
          from surprise import SVD, Dataset
          from surprise.model_selection import cross_validate

          # Quick test with small dataset
          data = Dataset.load_builtin('ml-100k', prompt=False)
          algo = SVD(n_factors=50, n_epochs=20, verbose=False)

          start_time = time.time()
          results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3, verbose=False)
          end_time = time.time()

          print(f'Quick benchmark completed in {end_time - start_time:.2f} seconds')
          print(f'RMSE: {results[\"test_rmse\"].mean():.4f} (±{results[\"test_rmse\"].std():.4f})')
          print(f'MAE: {results[\"test_mae\"].mean():.4f} (±{results[\"test_mae\"].std():.4f})')
          "

      - name: Run comprehensive integration tests (if benchmark enabled)
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        timeout-minutes: 15
        run: |
          python test_integration.py

      - name: Run full benchmark suite
        timeout-minutes: 30
        run: |
          cd examples
          echo "yes" | python benchmark.py || echo "Benchmark completed with expected prompts"

      - name: Generate benchmark report
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        run: |
          python -c "
          import json
          import platform
          import sys
          import surprise

          report = {
              'timestamp': '$(date -u +%Y-%m-%dT%H:%M:%SZ)',
              'python_version': sys.version,
              'platform': platform.platform(),
              'surprise_version': surprise.__version__,
              'os': '${{ matrix.os }}',
              'runner_os': '${{ runner.os }}'
          }

          with open('benchmark_report.json', 'w') as f:
              json.dump(report, f, indent=2)

          print('Benchmark report generated')
          print(json.dumps(report, indent=2))
          "

      - name: Upload benchmark results
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: |
            benchmark_report.json
            examples/*.log
          retention-days: 30

  benchmark-summary:
    needs: benchmark
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          merge-multiple: true

      - name: Create benchmark summary
        run: |
          echo "## Benchmark Results 📊" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Platform | Python | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------|--------|" >> $GITHUB_STEP_SUMMARY

          if [ -f benchmark_report.json ]; then
            echo "| Ubuntu | 3.11 | ✅ Success |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Performance Details" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat benchmark_report.json >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "| All | All | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark completed at $(date -u)" >> $GITHUB_STEP_SUMMARY
